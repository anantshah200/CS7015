# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vtxlxC1EQ3u1TSusdWPY_l8AI5jgOtLO
"""

# Program : Implementation of Convolutional Neural Networks using tensorflow on tiny ImageNet dataset
# Date : 14-3-2019
# Author : Anant Shah
# E-Mail : anantshah200@gmail.com

import tensorflow as tf
import matplotlib.pyplot as plot
import pickle 
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#from sklearn.metrics import accuracy_score
from tensorflow.python.framework import ops
import os

NUM_CLASSES = 20
NUM_FEATURES = 64*64*3
NUM_COMPONENTS = 64*64*3
H_I = 64
W_I = 64 
C_I = 3
MAX_POOL = 2

def get_specs() :
    "Function to get the specifications from the command line arguments"
    parser = argparse.ArgumentParser()
    parser.add_argument("--lr",type=float,help="initial learning rate of the algorithm")
    parser.add_argument("--batch_size",type=int,help="size of each mini-batch")
    parser.add_argument("--init",type=int,help="type of parameter initialization")
    parser.add_argument("--epochs",type=int,help="number of time steps for which we train our model")
    parser.add_argument("--save_dir",help="the directory where pickled model should be saved")
    parser.add_argument("--dataAugment",type=int,help="Data Augmentation used or not")
    parser.add_argument("--train",help="path to the training dataset")
    parser.add_argument("--val",help="path to the validation dataset")
    parser.add_argument("--test",help="path to the test dataset")
    
    args = parser.parse_args()
    return args

def get_data(train_path,val_path,test_path) :
    # Function to get the data for training
	  # Augment data so as to obtain more training examples by left shifting the pixels by 1
    train_data = pd.read_csv(train_path)
    val_data = pd.read_csv(val_path)
    test_data = pd.read_csv(test_path)
    train = np.array(train_data)
    val = np.array(val_data)
    test = np.array(test_data)

    N = train.shape[0]
    X_train = train[:,1:NUM_FEATURES+1].T / 255.
    assert X_train.shape == (NUM_FEATURES,N)

    Y_train = train[:,NUM_FEATURES+1,None].T
    Y_st = np.zeros((NUM_CLASSES-1,N)).astype(int)
    Y_train = np.vstack((Y_train,Y_st))
    init_index = np.where(Y_train[0,:] == 0)
    for i in range(1,NUM_CLASSES) :
      Y_train[i,np.where(Y_train[0][:] == int(i))] = 1
    Y_train[0,np.where(Y_train[0][:] != 0)] = 0
    Y_train[0,init_index] = 1
    Y_train = Y_train.T
    assert Y_train.shape == (N,NUM_CLASSES)

    X_val = val[:,1:NUM_FEATURES+1].T / 255.
    assert X_val.shape == (NUM_COMPONENTS,val.shape[0])

    Y_val = val[:,NUM_FEATURES+1,None].T
    Y_st = np.zeros((NUM_CLASSES-1,val.shape[0])).astype(int)
    Y_val = np.vstack((Y_val,Y_st))
    init_val = np.where(Y_val[0,:] == 0)
    for i in range(1,NUM_CLASSES) :
      Y_val[i,np.where(Y_val[0][:] == int(i))] = 1
    Y_val[0,np.where(Y_val[0][:] != 0)] = 0
    Y_val[0,init_val] = 1
    Y_val = Y_val.T
    assert Y_val.shape == (val.shape[0],NUM_CLASSES)

    X_test = test[:,1:NUM_FEATURES+1].T / 255.
    assert X_test.shape == (NUM_COMPONENTS,test.shape[0])
    
    N = X_train.shape[1]
    X_train = np.reshape(X_train.T,(N,H_I,W_I,C_I),order='C')
    N = X_val.shape[1]
    X_val = np.reshape(X_val.T,(N,H_I,W_I,C_I),order='C')
    N = X_test.shape[1]
    X_test = np.reshape(X_test.T,(N,H_I,W_I,C_I),order='C')
    
    data = {"X_train" : X_train,"Y_train" : Y_train,"X_val" : X_val,"Y_val" : Y_val,"X_test" : X_test}
    return data

def create_mini_batch(N, batch_size) :
	# Function to obtain randomized batch indices for the mini-batch optimization algorithm
	# Parameters -  N - number of training examples
	#		batch_size - The size of a batch(has to be a multiple of 5)
  indices = np.random.permutation(N)
  mini_batch_indices = [] # A list containing each set of indices
  
  for i in range(int(N/batch_size)) :
    mini_batch_indices.append(indices[i*batch_size:(i+1)*batch_size]) 
    
  mini_batch_indices.append(indices[int(N/batch_size)*batch_size:])
  return mini_batch_indices

def tensor_int(C,H,W) :
  # Function to generate placeholders for the inputs and outputs
  # Parameters : C - Number of channels in the input
  #              H - Height of the image in the input
  #              W - Widht of the image in the input
  
  X = tf.placeholder(dtype=tf.float32,shape=[None,H,W,C])
  Y = tf.placeholder(dtype=tf.float32,shape=[None,NUM_CLASSES])
  
  return X, Y

def initialize_parameters(init) :
  # Function to initialize the filters and biases
  # Based on the type of initialization : He or Xavier
  # Parameters : init - Type of initialization
  #              sizes - Size of each filter at each layer
  
  tf.set_random_seed(2)
  if init==1 :
    with tf.variable_scope("conv1") :
      W1 = tf.get_variable("W1",[5,5,3,64],initializer=tf.contrib.layers.xavier_initializer(seed=1))
    with tf.variable_scope("conv2") :
      W2 = tf.get_variable("W2",[5,5,64,64],initializer=tf.contrib.layers.xavier_initializer(seed=1))
    with tf.variable_scope("conv3") :
      W3 = tf.get_variable("W3",[3,3,64,64],initializer=tf.contrib.layers.xavier_initializer(seed=1))
    with tf.variable_scope("conv4") :
      W4 = tf.get_variable("W4",[3,3,64,64],initializer=tf.contrib.layers.xavier_initializer(seed=1))
    with tf.variable_scope("conv5") :
      W5 = tf.get_variable("W5",[3,3,64,64],initializer=tf.contrib.layers.xavier_initializer(seed=1))
    with tf.variable_scope("conv6") :
      W6 = tf.get_variable("W6",[3,3,64,128],initializer=tf.contrib.layers.xavier_initializer(seed=1))                   
    theta = {"W1" : W1, "W2" : W2, "W3" : W3, "W4" : W4, "W5" : W5, "W6" : W6}
  elif init==2 :
    with tf.variable_scope("conv1") :
      W1 = tf.get_variable("W1",[5,5,3,64],initializer=tf.contrib.layers.variance_scaling_initializer(uniform=True))
    with tf.variable_scope("conv2") :
      W2 = tf.get_variable("W2",[5,5,64,64],initializer=tf.contrib.layers.variance_scaling_initializer(uniform=True))
    with tf.variable_scope("conv3") :
      W3 = tf.get_variable("W3",[3,3,64,64],initializer=tf.contrib.layers.variance_scaling_initializer(uniform=True))
    with tf.variable_scope("conv4") :
      W4 = tf.get_variable("W4",[3,3,64,64],initializer=tf.contrib.layers.variance_scaling_initializer(uniform=True))
    with tf.variable_scope("conv5") :
      W5 = tf.get_variable("W5",[3,3,64,64],initializer=tf.contrib.layers.variance_scaling_initializer(uniform=True))
    with tf.variable_scope("conv6") :
      W6 = tf.get_variable("W6",[3,3,64,128],initializer=tf.contrib.layers.variance_scaling_initializer(uniform=True))                   
    theta = {"W1" : W1, "W2" : W2, "W3" : W3, "W4" : W4, "W5" : W5, "W6" : W6}
  return theta

def conv_layer(X,W,stride,padding,keep_prob_conv,op) :
  A = tf.nn.conv2d(X,W,strides=[1,stride,stride,1],padding=padding)
  H = tf.nn.leaky_relu(A,0.01)
  H = tf.contrib.layers.batch_norm(H,decay=0.999,is_training=op)
  H = tf.nn.dropout(H,rate=1-keep_prob_conv)
  return H

def max_pool(H) :
  return tf.nn.max_pool(H,ksize=[1,MAX_POOL,MAX_POOL,1],strides=[1,MAX_POOL,MAX_POOL,1],padding="VALID")

def feed_forward(X, theta, keep_prob_conv, keep_prob_fc,op) :
  # Function to perform the feed-forward operation
  # Parameters - X - Input Data
  #              theta - The input parameters
  #              pool_layers - A list containing of the layers at which we need to do max-pooling
  #              keep_prob_conv - The probability of keeping a neuron in the convolutional layers during dropout 
  #              keep_prob_fc - The probability of keeping a neuron in the fully connected layers during dropout
  #              num_fc - The number of neurons in each fully connected layer
  #              op - The type of operation either training or testing 
    
  S = 1 # Stride
  H = X
  
  # Layer 1
  H1 = conv_layer(H,theta["W1"],S,"SAME",keep_prob_conv,op)
  
  # Layer 2
  H2 = conv_layer(H1,theta["W2"],S,"SAME",keep_prob_conv,op)
  H2 = max_pool(H2)
  
  # Layer 3
  H3 = conv_layer(H2,theta["W3"],S,"SAME",keep_prob_conv,op)
  
  # Layer 4
  H4 = conv_layer(H3,theta["W4"],S,"SAME",keep_prob_conv,op)
  H4 = max_pool(H4)
  
  # Layer 5
  H5 = conv_layer(H4,theta["W5"],S,"SAME",keep_prob_conv,op)
  
  # Layer 6
  H6 = conv_layer(H5,theta["W6"],S,"VALID",keep_prob_conv,op)
  H6 = max_pool(H6)
  
  # Fully Connected Layers :
  H_fc = tf.reshape(H6,[-1,6272])
  
  H1_fc = tf.contrib.layers.fully_connected(H_fc,400,activation_fn=tf.nn.leaky_relu,weights_regularizer=tf.contrib.layers.l2_regularizer(0.003))
  H1_fc = tf.contrib.layers.batch_norm(H1_fc)
  H1_fc = tf.nn.dropout(H1_fc,rate=1-keep_prob_fc)
  H2_fc = tf.contrib.layers.fully_connected(H1_fc,400,activation_fn=tf.nn.leaky_relu,weights_regularizer=tf.contrib.layers.l2_regularizer(0.003))
  H2_fc = tf.contrib.layers.batch_norm(H2_fc)
  H2_fc = tf.nn.dropout(H2_fc,rate=1-keep_prob_fc)
  
  Y_hat = tf.contrib.layers.fully_connected(H2_fc,NUM_CLASSES,activation_fn=None,weights_regularizer=tf.contrib.layers.l2_regularizer(0.003)) # This is without the softmax function as that is acocunted for by the cost function
  
  return Y_hat

def cost(Y_hat,Y) :
  # Function to calculate the cost of the prediction
  # Parameters - Y_hat : The prediction of the model(before the softmax layer)
  #              Y : The true classes
  
  error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,logits=Y_hat))
  
  return error

def train(X_train,Y_train,X_val,Y_val,X_test,learning_rate,epochs,BATCH_SIZE,init,save_dir) :
  # Function to train the model
  # Parameters - X_train : The training data
  #              Y_train : The true classes for the training data
  #              X_val - The validation data
  #              Y_val - The true classes for the validation data
  #              learning_rate - The learning rate for the model
  #              epochs - Number of epochs for which we train the model
  #              batch_size - The size of a bacth in mini-batch gradient descent
  #              init - The type of initialization
  
  ops.reset_default_graph()
  tf.set_random_seed(2)
  
  train_costs = []
  N = int(X_train.shape[0])
  keep_prob_conv = tf.placeholder(tf.float32)
  keep_prob_fc = tf.placeholder(tf.float32)
  op = tf.placeholder(tf.bool)
  val_costs = []
  val_costs_plot = []
  lr = tf.placeholder(tf.float64)
  
  X, Y = tensor_int(C_I,H_I,W_I)
  
  theta = initialize_parameters(init)
  
  Y_hat = feed_forward(X,theta,keep_prob_conv,keep_prob_fc,op,train)
  
  train_cost = cost(Y_hat,Y)
  
  # We know perform the back-prop algorithm to update the filters and weights in the fully-connected layers
  backprop = tf.train.AdamOptimizer(learning_rate=lr).minimize(train_cost)
  
  comp = tf.equal(tf.argmax(Y_hat,1),tf.argmax(Y,1))
  acc = tf.reduce_mean(tf.cast(comp,tf.float32))
  cor_ind = tf.argmax(Y_hat,1)
  
  saver = tf.train.Saver()
  #if not os.path.exists(save_dir) :
  #  os.makedirs(save_dir)
  
  patience = 5
  val_count = 0
  i = 0
  end_epoch = 0
  
  with tf.Session() as sess :
    
    sess.run(tf.global_variables_initializer())
    
    for epoch in range(1,epochs+1) :
      
      indices = np.random.permutation(N)
      train_cost_epoch = 0.0
      num_batches = N//BATCH_SIZE
      
      for batch in range(num_batches):
        
        X_plac = X_train[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,N)]
        Y_plac = Y_train[batch*BATCH_SIZE:min((batch+1)*BATCH_SIZE,N)]
        opt, train_cost_temp = sess.run([backprop, train_cost],feed_dict={X:X_plac,Y:Y_plac,keep_prob_conv:0.8,keep_prob_fc:0.5,op:True,lr:learning_rate})
        train_cost_epoch += train_cost_temp / num_batches
        
      accuracy, val_cost_temp = sess.run([acc, train_cost], feed_dict={X : X_val, Y : Y_val, keep_prob_conv:1.0, keep_prob_fc:1.0,op:True})
      train_costs.append(train_cost_epoch)
      val_costs_plot.append(val_cost_temp)
      
      # Keep a counter for early stopping
      
      if i>=1 :
        if accuracy < val_costs[i-1] :
          val_count = val_count + 1
        else :
          val_count = 0
        val_costs.append(accuracy)
      else :
        val_costs.append(accuracy)
      i = i + 1
      
      # If the cost keeps on increasing for patience epochs, load the model at a previous timestep and stop training
      if val_count == patience :
        end_epoch = epoch-patience
        saver.restore(sess,save_dir+"weights-"+str(end_epoch)+".ckpt")
        break
      
      if epoch%10==0 :
        learning_rate = learning_rate/2
      
      saver.save(sess,save_dir+"weights-"+str(epoch)+".ckpt")
      print("Epoch :"+str(epoch)+"Accuracy : "+str(accuracy))
    
    if end_epoch == 0 :
      end_epoch = epoch
    
    pred = sess.run(cor_ind, feed_dict={X : X_test, keep_prob_conv: 1.0, keep_prob_fc: 1.0, op:True})
    pred = pred.reshape(pred.shape[0],1)
    index = (np.arange(0,X_test.shape[0]).T).reshape(X_test.shape[0],1)
    pred = np.hstack((index,pred))
    pd.DataFrame(pred).to_csv("predictions.csv", header=["id","label"],index=False)
    
  return theta, train_costs, val_costs_plot, end_epoch

def augment_data(X_t,H,W,C) :
  # Function to augment the training data
  
  X = tf.placeholder(dtype=tf.float32,shape=[None,H,W,C])
  
  flip_1 = tf.image.flip_left_right(X)
  rot_1 = tf.image.flip_up_down(X)
  X_aug = tf.concat([X,flip_1,rot_1],axis=0)

  with tf.Session() as sess :
    X_aug = sess.run(X_aug,feed_dict={X:X_t})
  
    return X_aug

def to_tensor(A,H) :
  # Function to convert a list back to the tensor shape
  return tf.reshape(tf.convert_to_tensor(A),shape=tf.shape(H))

def guided_backprop(image,save_dir,epoch) :
  # Function to demonstarte guided backprop on the trained model
  # Parameters - X : The plain starting image
  #              H - the height of the image
  #              W - the widht of the image
  #              C - the number of channels in the image
  #              theta - The weights of the trained model
  S = 1
  
  tf.reset_default_graph()
  theta, x, y, train, lr, pred, y_hat, cost, optimizer, correct_prediction, accuracy, init,keep_prob_conv, keep_prob_fc, op = make_graph()
  img = tf.placeholder(dtype=tf.float32,shape=[None,H_I,W_I,C_I])
  
  A1 = tf.nn.conv2d(img,theta["W1"],strides=[1,S,S,1],padding="SAME")
  H1 = tf.nn.relu(A1)
  
  A2 = tf.nn.conv2d(H1,theta["W2"],strides=[1,S,S,1],padding="SAME")
  H2 = tf.nn.relu(A2)
  H2_max_pool = tf.nn.max_pool(H2,ksize=[1,MAX_POOL,MAX_POOL,1],strides=[1,MAX_POOL,MAX_POOL,1],padding="VALID")
  
  A3 = tf.nn.conv2d(H2_max_pool,theta["W3"],strides=[1,S,S,1],padding="SAME")
  H3 = tf.nn.relu(A3)
  
  A4 = tf.nn.conv2d(H3,theta["W4"],strides=[1,S,S,1],padding="SAME")
  H4 = tf.nn.relu(A4)
  H4_max_pool = tf.nn.max_pool(H4,ksize=[1,MAX_POOL,MAX_POOL,1],strides=[1,MAX_POOL,MAX_POOL,1],padding="VALID")
  
  A5 = tf.nn.conv2d(H4_max_pool,theta["W5"],strides=[1,S,S,1],padding="SAME")
  H5 = tf.nn.relu(A5)
  
  A6 = tf.nn.conv2d(H5,theta["W6"],strides=[1,S,S,1],padding="VALID")
  H6 = tf.nn.relu(A6)
  
  neur = tf.math.reduce_sum(H6,axis=3)
  neur_act = tf.gather_nd(neur,[[0,4,6]])
  
  dH6 = to_tensor(tf.gradients(neur_act,H6),H6)
  dA6 = to_tensor(tf.gradients(H6,A6,grad_ys=dH6),A6)
  dA6 = tf.where(dA6>0.0,dA6,tf.fill(tf.shape(dA6),0.0))
  
  dH5 = to_tensor(tf.gradients(A6,H5,grad_ys=dA6,stop_gradients=theta["W6"]),H5)
  dH5 = tf.where(dH5 > 0.0,dH5,tf.fill(tf.shape(dH5),0.0))
  dA5 = to_tensor(tf.gradients(H5,A5,grad_ys=dH5),A5)
  dA5 = tf.where(dA5>0.0,dA5,tf.fill(tf.shape(dA5),0.0))
  
  dH4_max_pool = to_tensor(tf.gradients(A5,H4_max_pool,grad_ys=dA5,stop_gradients=theta["W5"]),H4_max_pool)
  dH4_max_pool = tf.where(dH4_max_pool > 0.0,dH4_max_pool,tf.fill(tf.shape(dH4_max_pool),0.0))
  dH4 = to_tensor(tf.gradients(H4_max_pool,H4,grad_ys=dH4_max_pool),H4)
  dH4 = tf.where(dH4>0.0,dH4,tf.fill(tf.shape(dH4),0.0))
  dA4 = to_tensor(tf.gradients(H4,A4,grad_ys=dH4),A4)
  dA4 = tf.where(dA4>0.0,dA4,tf.fill(tf.shape(dA4),0.0))
  
  dH3 = to_tensor(tf.gradients(A4,H3,grad_ys=dA4,stop_gradients=theta["W4"]),H3)
  dH3 = tf.where(dH3>0.0,dH3,tf.fill(tf.shape(dH3),0.0))
  dA3 = to_tensor(tf.gradients(H3,A3,grad_ys=dH3),A3)
  dA3 = tf.where(dA3>0.0,dA3,tf.fill(tf.shape(dA3),0.0))
  
  dH2_max_pool = to_tensor(tf.gradients(A3,H2_max_pool,grad_ys=dA3,stop_gradients=theta["W3"]),H2_max_pool)
  dH2_max_pool = tf.where(dH2_max_pool > 0.0,dH2_max_pool,tf.fill(tf.shape(dH2_max_pool),0.0))
  dH2 = to_tensor(tf.gradients(H2_max_pool,H2,grad_ys=dH2_max_pool),H2)
  dH2 = tf.where(dH2>0.0,dH2,tf.fill(tf.shape(dH2),0.0))
  dA2 = to_tensor(tf.gradients(H2,A2,grad_ys=dH2),A2)
  dA2 = tf.where(dA2>0.0,dA2,tf.fill(tf.shape(dA2),0.0))

  dH1 = to_tensor(tf.gradients(A2,H1,grad_ys=dA2,stop_gradients=theta["W2"]),H1)
  dH1 = tf.where(dH1>0.0,dH1,tf.fill(tf.shape(dH1),0.0))
  dA1 = to_tensor(tf.gradients(H1,A1,grad_ys=dH1),A1)
  dA1 = tf.where(dA1>0.0,dA1,tf.fill(tf.shape(dA1),0.0))
  
  dimg = to_tensor(tf.gradients(A1,img,grad_ys=dA1,stop_gradients=theta["W1"]),img)
  
  saver = tf.train.Saver()
  with tf.Session() as sess :
    saver.restore(sess,save_dir+'weights-'+str(epoch)+".ckpt")
    #K = dH5.eval(session=sess,feed_dict={img:image})
    #print(K)
    
    for i in range(1,50) :
      der_img,m = sess.run([dimg,neur_act],feed_dict={img:image})
      print(m)
      der_img = der_img / (np.std(der_img)+1e-8)
      image = image + 0.008*der_img
    w_min = np.min(image[0])
    w_max = np.max(image[0])
    new_img = (image[0]-w_min)/(w_max-w_min)
    plt.imshow(new_img,cmap='Greys')

def make_graph():
  
  tf.reset_default_graph()
  
  theta = initialize_parameters(1)

  x = tf.placeholder( tf.float32, ( None, H_I, W_I, C_I ) )
  y = tf.placeholder( tf.float32, ( None, NUM_CLASSES ) )
  train = tf.placeholder( tf.bool )
  lr = tf.placeholder( tf.float32, shape = [] )
  keep_prob_conv = tf.placeholder(tf.float32)
  keep_prob_fc = tf.placeholder(tf.float32)
  op = tf.placeholder(tf.bool)
  
  pred = feed_forward(x,theta,keep_prob_conv,keep_prob_fc,op,train)
  
  y_hat = tf.nn.softmax( pred )
  
  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
  
  optimizer = tf.train.AdamOptimizer(learning_rate=lr, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8).minimize(cost)
  
  correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
  
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  
  init = tf.global_variables_initializer()
  
  return theta, x, y, train, lr, pred, y_hat, cost, optimizer, correct_prediction, accuracy, init, keep_prob_conv, keep_prob_fc, op

def fool_network(saved_model, x_orig, y_orig, fake_as, num_iter = 100 ):
  
  tf.reset_default_graph()
  
  theta, x, y, train, lr, pred, y_hat, cost, optimizer, correct_prediction, accuracy, init,keep_prob_conv, keep_prob_fc, op = make_graph()
  
  sess2 = tf.Session()
  
  learning_rate = 0.01
  
  saver2 = tf.train.Saver()
  
  saver2.restore(sess2, saved_model)
  
  print( "restore successful" )
  
  # val_loss, val_acc = sess2.run([cost, accuracy], feed_dict={x: X_val,y : Y_val, training: True, lr: learning_rate })
  # print("\n Step: " + str(t) + " LR: " + str(learning_rate) + ", Validation Loss= " + "{:.6f}".format(val_loss) + ", Validation Accuracy= " + "{:.5f}".format(val_acc) + "\n" )
  
  y_pred = sess2.run(y_hat, feed_dict={x: x_orig, y: y_orig, train: False, op:True, lr: learning_rate, keep_prob_conv:1, keep_prob_fc:1})
  print("Pred"+str(y_pred))
  predicted_class = y_hat[:, fake_as]
  
  gradient_op = tf.gradients(predicted_class, x)
  
  #for k in range(x_class_i.shape[0]):
   #   print( y_pred[k] )
  
  for k in range( num_iter ):
      gradients = sess2.run(gradient_op, {x: x_orig, y: y_orig, train: False, op:True, lr: learning_rate, keep_prob_conv:1.0, keep_prob_fc:1.0})
      gradients = gradients / (np.std(gradients) + 1e-8)
      print(gradients)
      x_orig = x_orig + (gradients[0] * 0.05) # Adjusting by less than a full step each loop
    
      if ( k % int(num_iter / 10) == 0 ):
        plt.imshow(x_orig.reshape([H_I, W_I, C_I]))
        plt.show()
        print(y_hat.eval(session = sess2 ,feed_dict = {x: x_orig, y: y_orig, train: False, op:True, lr: learning_rate, keep_prob_conv:1.0, keep_prob_fc:1.0}))
  
      # Now, classify the adjusted img_noise
  
  sess2.close()
  
  tf.reset_default_graph()

def test_acc(X_val,Y_val,save_dir,epoch) :
  theta, x, y, train, lr, pred, y_hat, cost, optimizer, correct_prediction, accuracy, init,keep_prob_conv, keep_prob_fc, op = make_graph()
  sess = tf.Session()
  
  saver = tf.train.Saver()
  
  saver.restore(sess,save_dir+"weights-"+str(epoch)+".ckpt")
  
  acc = sess.run(accuracy,feed_dict={x:X_val,y:Y_val,keep_prob_conv:1.0,keep_prob_fc:1.0,op:True})
  print(acc)

args = get_specs()
lr = args.lr
batch_size = args.batch_size
init = args.init
epochs = args.epochs
save_dir = args.save_dir
dataAugment = args.dataAugment
train_path = args.train
val_path = args.val
test_path = args.test
data = get_data(train_path,val_path,test_path)
# data dictionary contains the data in matrix form. We need to convert it into a 64*64*3 image
X_train = data["X_train"]
X_val = data["X_val"]
X_test = data["X_test"]
Y_train = data["Y_train"]
Y_val = data["Y_val"]
if dataAugment==1 :
	X_train = augment_data(X_train,H_I,W_I,C_I)
	X_train = np.asarray(X_train)
	ii = np.random.permutation(X_train.shape[0])
	X_train = X_train[ii]
	Y_train = np.vstack((Y_train, Y_train, Y_train))
	Y_train = Y_train[ii]
vals_lr = []
train_lr = []
# It 1
theta, train_costs, val_costs, end_epoch = train(X_train,Y_train,X_val,Y_val,X_test,learning_rate=lr,epochs=epochs,BATCH_SIZE=batch_size,init=init,save_dir=save_dir)
#test_acc(X_val,Y_val,"",20)
#theta, train_costs, val_costs, end_epoch = train(X_train,Y_train,X_val,Y_val,X_test,learning_rate,epochs,batch_size,init,save_dir)
#train_lr.append(train_costs)
#vals_lr.append(val_costs)
image = X_train[1456]
y = Y_train[199,:]
y = np.reshape(y,(1,20))
plt.figure(1)
plt.imshow(image)
image = np.expand_dims(image,axis=0)
plt.imshow(image[0])
#image = np.random.randn(1,H_I,W_I,C_I)
guided_backprop(image,save_dir="",epoch=20)
#fool_network("weights-20.ckpt",image,y,17)
